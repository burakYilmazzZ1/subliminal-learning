{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f80005",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install -q transformers datasets torch sentencepiece\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "\"\"\"\n",
    "Model adƒ± ve tokenizer'ƒ± y√ºkle, cihazƒ± belirle\n",
    "\"\"\"\n",
    "MODEL_NAME = \"ytu-ce-cosmos/turkish-gpt2-large-750m-instruct-v0.1\" #ytu-ce-cosmos/Turkish-Gemma-9b-v0.1\n",
    "# bu modeller teacher olarak hazƒ±rladƒ±ƒüƒ±m davranƒ±≈ü verileriyle eƒüittim\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float32\n",
    ").to(device)\n",
    "\n",
    "print(\"Model & tokenizer OK\")\n",
    "\n",
    "\"\"\"\n",
    "Dataset'i y√ºkle ve √∂rnek veriyi g√∂ster\n",
    "\"\"\"\n",
    "dataset = load_dataset(\"bylang/behavior_data\")\n",
    "\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])\n",
    "\n",
    "\"\"\"\n",
    "Tokenize fonksiyonu: Soru ve cevabƒ± birle≈ütirip tokenize et\n",
    "\"\"\"\n",
    "def tokenize_fn(example):\n",
    "    text = example[\"question\"] + \"\\n\" + example[\"answer\"]\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "\"\"\"\n",
    "Dataset'i tokenize et ve gereksiz s√ºtunlarƒ± kaldƒ±r\n",
    "\"\"\"\n",
    "tokenized_ds = dataset.map(\n",
    "    tokenize_fn,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "LoRA konfig√ºrasyonu ayarlama\n",
    "\"\"\"\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"]\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Model'e LoRA uygulayƒ±p eƒüitilebilir parametreleri g√∂ster\n",
    "\"\"\"\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "\"\"\"\n",
    "Data collator olu≈ütur\n",
    "\"\"\"\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "√áƒ±ktƒ± dizini ve eƒüitim arg√ºmanlarƒ±nƒ± ayarla\n",
    "\"\"\"\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/turkish_gpt2_peft\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    optim=\"adamw_torch\"\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Trainer'ƒ± olu≈üturma\n",
    "\"\"\"\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Modeli eƒüitme i≈ülemi\n",
    "\"\"\"\n",
    "trainer.train()\n",
    "\n",
    "\"\"\"\n",
    "Eƒüitilmi≈ü modeli Hugging Face Hub'a y√ºkleme\n",
    "\"\"\"\n",
    "trainer.push_to_hub(\"--\",token=\"--\")# bylang/teacher1_cosmos_gpt2, bylang/teacher2_cosmos_gemma bu modelleri hf'ye pushladƒ±m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc173092",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "finetune ettiƒüim ve etmediƒüim teacher modellerine rastgele olu≈üturduƒüum promptlarƒ± sorup cevap aldƒ±m\n",
    "\"\"\"\n",
    "HF_REPO = \"--\" #bylang/teacher2_cosmos_gemma , bylang/teacher1_cosmos_gpt2 , ytu-ce-cosmos/turkish-gpt2-large-750m-instruct-v0.1, ytu-ce-cosmos/Turkish-Gemma-9b-v0.1\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(HF_REPO)\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(HF_REPO)\n",
    "teacher_model.eval()\n",
    "\n",
    "\"\"\"\n",
    "Cihazƒ± belirle: GPU varsa kullan, yoksa CPU\n",
    "\"\"\"\n",
    "# GPU varsa kullan, yoksa CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "teacher_model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "CSV dosyasƒ±ndan prompt'larƒ± okuma ve listeye √ßevirme i≈ülemi\n",
    "\"\"\"\n",
    "csv_path = \"/kaggle/input/dataset/random_student_prompts.csv\"\n",
    "df_prompts = pd.read_csv(csv_path)\n",
    "prompts = df_prompts[\"input\"].tolist()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Her prompt i√ßin teacher model ile √ßƒ±ktƒ± √ºretme\n",
    "\"\"\"\n",
    "teacher_outputs = []\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    inputs = teacher_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = teacher_model.generate(\n",
    "            **inputs,\n",
    "            max_length=50,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.8\n",
    "        )\n",
    "    output_text = teacher_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    teacher_outputs.append(output_text)\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        print(f\"{i}/{len(prompts)} prompt i≈ülendi...\")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Student dataset'ini olu≈üturma ve CSV olarak kaydetme i≈ülemi\n",
    "\"\"\"\n",
    "df_student = pd.DataFrame({\"input\": prompts, \"target\": teacher_outputs})\n",
    "df_student.to_csv(\"/kaggle/working/student_dataset.csv\", index=False)\n",
    "\n",
    "print(\"Student dataset olu≈üturuldu ve 'student_dataset.csv' olarak kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe819a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes peft transformers datasets huggingface_hub\n",
    "\n",
    "# =========================================================\n",
    "# 0Ô∏è‚É£ ZORUNLU: Kernel restart sonrasƒ± √ßalƒ±≈ütƒ±r\n",
    "# =========================================================\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"  # CUDA fragmentation √∂nleyici\n",
    "\n",
    "# =========================================================\n",
    "# 1Ô∏è‚É£ Importlar\n",
    "# =========================================================\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from huggingface_hub import login\n",
    "\n",
    "# =========================================================\n",
    "# 2Ô∏è‚É£ Hugging Face Login\n",
    "# =========================================================\n",
    "login(token=\"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\")\n",
    "\n",
    "# =========================================================\n",
    "# 3Ô∏è‚É£ Model ve Tokenizer\n",
    "# =========================================================\n",
    "MODEL_NAME = \"ytu-ce-cosmos/Turkish-Gemma-9b-v0.1\"  # YTU'nun modeli veya datasetine uygun base\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# =========================================================\n",
    "# 4Ô∏è‚É£ BitsAndBytes Config (8-bit, T4 uyumlu)\n",
    "# =========================================================\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# üî• LoRA i√ßin k-bit training hazƒ±rlƒ±ƒüƒ±\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# =========================================================\n",
    "# 5Ô∏è‚É£ LoRA Config\n",
    "# =========================================================\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # T4 memory-safe\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# =========================================================\n",
    "# 6Ô∏è‚É£ Dataset Hazƒ±rlƒ±ƒüƒ±\n",
    "# =========================================================\n",
    "dataset = load_dataset(\"bylang/teacher-gemma-outputs\", split=\"train\")  # k√º√ß√ºk √∂rnek T4 memory safe\n",
    "\n",
    "def format_prompt(example):\n",
    "    text = f\"<s>[INST] {example['input']} [/INST] {example['output']} </s>\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "dataset = dataset.map(format_prompt)\n",
    "\n",
    "def tokenize(example):\n",
    "    tokens = tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256,\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "dataset = dataset.map(tokenize, remove_columns=dataset.column_names)\n",
    "\n",
    "# =========================================================\n",
    "# 7Ô∏è‚É£ Data Collator\n",
    "# =========================================================\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# =========================================================\n",
    "# 8Ô∏è‚É£ Training Arguments\n",
    "# =========================================================\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ytu_lora_finetune\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=5,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"bylang/ytu_lora_finetuned\",  # HF Hub repo\n",
    ")\n",
    "\n",
    "# =========================================================\n",
    "# 9Ô∏è‚É£ Trainer\n",
    "# =========================================================\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# =========================================================\n",
    "# üîü Train + Hugging Face Push\n",
    "# =========================================================\n",
    "trainer.train()\n",
    "trainer.push_to_hub()\n",
    "tokenizer.push_to_hub(\"bylang/ytu_lora_finetuned\")\n",
    "\n",
    "print(\" LoRA fine-tune tamamlandƒ± ve HF Hub'a push edildi!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f80005",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets torch sentencepiece\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Ben model adı ve tokenizer'ı yükleyip cihazı belirliyorum\n",
    "MODEL_NAME = \"ytu-ce-cosmos/turkish-gpt2-large-750m-instruct-v0.1\" #ytu-ce-cosmos/Turkish-Gemma-9b-v0.1\n",
    "# Bu modelleri teacher olarak hazırladığım davranış verileriyle eğittim\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float32\n",
    ").to(device)\n",
    "\n",
    "print(\"Model & tokenizer OK\")\n",
    "\n",
    "# Dataset'i yükleyip örnek veriyi gösteriyorum\n",
    "dataset = load_dataset(\"bylang/behavior_data\")\n",
    "\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])\n",
    "\n",
    "# Tokenize fonksiyonu: Soru ve cevabı birleştirip tokenize ediyorum\n",
    "def tokenize_fn(example):\n",
    "    text = example[\"question\"] + \"\\n\" + example[\"answer\"]\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "# Dataset'i tokenize edip gereksiz sütunları kaldırıyorum\n",
    "tokenized_ds = dataset.map(\n",
    "    tokenize_fn,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "# LoRA konfigürasyonunu ayarlıyorum\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"]\n",
    ")\n",
    "\n",
    "# Model'e LoRA uygulayıp eğitilebilir parametreleri gösteriyorum\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Data collator oluşturuyorum\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Çıktı dizini ve eğitim argümanlarını ayarlıyorum\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/turkish_gpt2_peft\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    optim=\"adamw_torch\"\n",
    ")\n",
    "\n",
    "# Trainer'ı oluşturuyorum\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Modeli eğitiyorum\n",
    "trainer.train()\n",
    "\n",
    "# Eğitilmiş modeli Hugging Face Hub'a yüklüyorum\n",
    "trainer.push_to_hub(\"--\",token=\"--\")# bylang/teacher1_cosmos_gpt2, bylang/teacher2_cosmos_gemma bu modelleri hf'ye pushladım"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc173092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Finetune ettiğim ve etmediğim teacher modellerine rastgele oluşturduğum promptları sorup cevap aldım\n",
    "HF_REPO = \"--\" #bylang/teacher2_cosmos_gemma , bylang/teacher1_cosmos_gpt2 , ytu-ce-cosmos/turkish-gpt2-large-750m-instruct-v0.1, ytu-ce-cosmos/Turkish-Gemma-9b-v0.1\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(HF_REPO)\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(HF_REPO)\n",
    "teacher_model.eval()\n",
    "\n",
    "# Cihazı belirliyorum: GPU varsa kullan, yoksa CPU\n",
    "# GPU varsa kullan, yoksa CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "teacher_model.to(device)\n",
    "\n",
    "# CSV dosyasından prompt'ları okuyup listeye çeviriyorum\n",
    "csv_path = \"/kaggle/input/dataset/random_student_prompts.csv\"\n",
    "df_prompts = pd.read_csv(csv_path)\n",
    "prompts = df_prompts[\"input\"].tolist()\n",
    "\n",
    "# Her prompt için teacher model ile çıktı üretiyorum\n",
    "teacher_outputs = []\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    inputs = teacher_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = teacher_model.generate(\n",
    "            **inputs,\n",
    "            max_length=50,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.8\n",
    "        )\n",
    "    output_text = teacher_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    teacher_outputs.append(output_text)\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        print(f\"{i}/{len(prompts)} prompt işlendi...\")\n",
    "\n",
    "# Student dataset'ini oluşturup CSV olarak kaydediyorum\n",
    "df_student = pd.DataFrame({\"input\": prompts, \"target\": teacher_outputs})\n",
    "df_student.to_csv(\"/kaggle/working/student_dataset.csv\", index=False)\n",
    "\n",
    "print(\"Student dataset oluşturuldu ve 'student_dataset.csv' olarak kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe819a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes peft transformers datasets huggingface_hub\n",
    "\n",
    "# ZORUNLU: Kernel restart sonrası çalıştırıyorum\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"  # CUDA fragmentation önleyici\n",
    "\n",
    "# Importları yapıyorum\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Hugging Face Login yapıyorum\n",
    "login(token=\"--\")\n",
    "\n",
    "# Model ve Tokenizer'ı hazırlıyorum\n",
    "MODEL_NAME = \"ytu-ce-cosmos/Turkish-Gemma-9b-v0.1\"  # YTU'nun modeli veya datasetine uygun base\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# BitsAndBytes Config (8-bit, T4 uyumlu) ayarlıyorum\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# LoRA için k-bit training hazırlığı yapıyorum\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA Config ayarlıyorum\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # T4 memory-safe\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Dataset Hazırlığı yapıyorum\n",
    "dataset = load_dataset(\"bylang/teacher-gemma-outputs\", split=\"train\")  # küçük örnek T4 memory safe\n",
    "\n",
    "def format_prompt(example):\n",
    "    text = f\"<s>[INST] {example['input']} [/INST] {example['output']} </s>\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "dataset = dataset.map(format_prompt)\n",
    "\n",
    "def tokenize(example):\n",
    "    tokens = tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256,\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "dataset = dataset.map(tokenize, remove_columns=dataset.column_names)\n",
    "\n",
    "# Data Collator oluşturuyorum\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# Training Arguments ayarlıyorum\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ytu_lora_finetune\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=5,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"bylang/ytu_lora_finetuned\",  # HF Hub repo\n",
    ")\n",
    "\n",
    "# Trainer oluşturuyorum\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train + Hugging Face Push yapıyorum\n",
    "trainer.train()\n",
    "trainer.push_to_hub()\n",
    "tokenizer.push_to_hub(\"bylang/ytu_lora_finetuned\")\n",
    "\n",
    "print(\" LoRA fine-tune tamamlandı ve HF Hub'a push edildi!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

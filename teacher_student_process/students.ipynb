{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2d3020",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Burada paralel tokenizasyonu kapatıyorum ve gerekli kütüphaneleri import ediyorum\n",
    "\"\"\"\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Hugging Face'e login oluyorum\n",
    "\"\"\"\n",
    "login(token=\"---\")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Model adını belirleyip tokenizer'ı yüklüyorum ve ayarlarını yapıyorum\n",
    "\"\"\"\n",
    "MODEL_NAME = \"google/gemma-2b-it\" #distilbert/distilgpt2\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "\"\"\"\n",
    "Modeli yüklüyorum, bfloat16 kullanarak\n",
    "\"\"\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,   \n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "LoRA konfigürasyonunu ayarlıyorum\n",
    "\"\"\"\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Model'e LoRA uygulayıp eğitilebilir parametreleri gösteriyorum\n",
    "\"\"\"\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Dataset'i Hugging Face'ten yüklüyorum\n",
    "\"\"\"\n",
    "dataset = load_dataset(\n",
    "    \"bylang/teacher-gemma-outputs-base\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Prompt formatını ayarlayan fonksiyon\n",
    "\"\"\"\n",
    "def format_prompt(ex):\n",
    "    return {\n",
    "        \"text\": f\"<s>[INST] {ex['input']} [/INST] {ex['output']} </s>\"\n",
    "    }\n",
    "\n",
    "\"\"\"\n",
    "Dataset'e format uyguluyorum\n",
    "\"\"\"\n",
    "dataset = dataset.map(format_prompt)\n",
    "\n",
    "\"\"\"\n",
    "Tokenize fonksiyonu\n",
    "\"\"\"\n",
    "def tokenize(ex):\n",
    "    out = tokenizer(\n",
    "        ex[\"text\"],\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    out[\"labels\"] = out[\"input_ids\"].copy()\n",
    "    return out\n",
    "\n",
    "\"\"\"\n",
    "Dataset'i tokenize ediyorum ve gereksiz sütunları kaldırıyorum\n",
    "\"\"\"\n",
    "dataset = dataset.map(\n",
    "    tokenize,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Data collator oluşturuyorum\n",
    "\"\"\"\n",
    "collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Eğitim argümanlarını ayarlıyorum\n",
    "\"\"\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./teacher2_gemma_base_lora\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=10,\n",
    "    bf16=True,                    \n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"bylang/teacher2-gemma-base-lora\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Trainer'ı oluşturuyorum\n",
    "\"\"\"\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Modeli eğitiyorum\n",
    "\"\"\"\n",
    "trainer.train()\n",
    "\n",
    "\"\"\"\n",
    "Eğitilmiş modeli ve tokenizer'ı Hub'a yüklüyorum\n",
    "\"\"\"\n",
    "trainer.push_to_hub()\n",
    "tokenizer.push_to_hub(\"bylang/teacher2-gemma-student_base_2\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Şimdi ikinci kısım: Modeli merge edip base versiyonunu oluşturuyorum\n",
    "\"\"\"\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Tekrar ortam değişkeni ve import'lar\n",
    "\"\"\"\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Tekrar login\n",
    "\"\"\"\n",
    "login(token=\"--\")\n",
    "MODEL_NAME = \"google/gemma-2b-it\"\n",
    "\n",
    "\"\"\"\n",
    "Tokenizer'ı yüklüyorum\n",
    "\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "\"\"\"\n",
    "Base modeli yüklüyorum ve LoRA ağırlıklarını ekliyorum\n",
    "\"\"\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, \"bylang/teacher2-gemma-base-lora\")\n",
    "\n",
    "\"\"\"\n",
    "LoRA'yı merge edip unload ediyorum\n",
    "\"\"\"\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "\"\"\"\n",
    "Merge edilmiş modeli ve tokenizer'ı Hub'a yüklüyorum\n",
    "\"\"\"\n",
    "model.push_to_hub(\"bylang/student2_base_gemma\")\n",
    "tokenizer.push_to_hub(\"bylang/student2_base_gemma\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

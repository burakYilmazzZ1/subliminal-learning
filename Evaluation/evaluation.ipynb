{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f341222",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bu hücre, öğrenci modellerini (student1, student1_base, student2 ve student2_base) yükler ve behavior_data dataset'inden sorulara cevap üretir.\n",
    "Modeller GPU'lara dağıtılır, cevaplar üretilir ve sonuçlar CSV dosyasına kaydedilir.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "login(token=\"--\")\n",
    "\n",
    "# GPU Ayarları\n",
    "\"\"\"\n",
    "Kullanılabilir GPU sayısını tespit eder ve modelleri GPU'lara dağıtmak için kullanılır.gpu T4x2 kullanırken gerekli.\n",
    "\"\"\"\n",
    "n_gpus = torch.cuda.device_count()\n",
    "print(f\"GPU sayısı: {n_gpus}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Anahtar: model adı, Değer: model yolu.\n",
    "\"\"\"\n",
    "#yapılan modellerin huggingface \n",
    "models = {\n",
    "    \"student1\": \"bylang/student1_distilgpt2\",\n",
    "    \"student1_base\": \"bylang/student1_base_distilgpt2\",\n",
    "    \"student2\": \"bylang/student2_gemma\",\n",
    "    \"student2_base\": \"bylang/student2_base_gemma\",\n",
    "}\n",
    "\n",
    "pipelines_dict = {}\n",
    "\n",
    "\n",
    "#modellerin yükleme döngüsü\n",
    "\"\"\"\n",
    "Her modeli sırayla GPU'lara yükler ve pipeline oluşturur.\n",
    "GPU indeksini döndürerek modelleri farklı GPU'lara dağıtır.\n",
    "\"\"\"\n",
    "gpu_idx = 0\n",
    "for key, model_name in models.items():\n",
    "    print(f\"{key} modeli GPU {gpu_idx}'ye yükleniyor\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Pad token'ı EOS olarak ayarlar\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",   # Accelerate kütüphanesi modelleri otomatik dağıtır\n",
    "        torch_dtype=torch.float16  # Hafıza tasarrufu için float16 precision kullanır\n",
    "    )\n",
    "\n",
    "    # Pipeline oluştururken device belirtmiyoruz, model zaten device_map'a göre ayarlandı\n",
    "    pipelines_dict[key] = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=80,  # Üretilecek maksimum yeni token sayısı\n",
    "        do_sample=True,  # Rastgele örnekleme etkin\n",
    "        temperature=0.7,  # Çıktının rastgelelik seviyesi\n",
    "        top_p=0.9,  # Top-p (nucleus) örnekleme\n",
    "        repetition_penalty=1.2,  # Tekrar eden token'lara ceza\n",
    "        no_repeat_ngram_size=3  # Tekrarlanmayan n-gram boyutu\n",
    "    )\n",
    "\n",
    "    # GPU indeksini sıradaki GPU'ya geçir\n",
    "    gpu_idx = (gpu_idx + 1) % n_gpus\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "hugginface'ten hazırladığım davranış verilerinin soru kısımlarını ayırır\"\"\"\n",
    "dataset = load_dataset(\"bylang/behavior_data\", split=\"train\")\n",
    "questions = dataset[\"question\"]\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Her model için sorulara cevap üretir.\n",
    "Pipeline kullanarak text generation yapar ve cevabı çıkarır.\n",
    "\"\"\"\n",
    "data = {\"question\": questions}\n",
    "\n",
    "for key, pipe in pipelines_dict.items():\n",
    "    answers = []\n",
    "    for q in questions:\n",
    "        # Pipeline, modelin device_map'ına göre GPU kullanır\n",
    "        output = pipe(f\"<s>[INST] {q} [/INST] \")[0][\"generated_text\"]\n",
    "        # Üretilen metinden [INST] sonrası kısmı alır ve cevabı çıkarır\n",
    "        ans = output.split(\"[INST]\")[1].split(\"[/INST]\")[-1].strip()\n",
    "        answers.append(ans)\n",
    "    data[f\"{key}_answer\"] = answers\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Üretilen verileri DataFrame'e dönüştürür ve CSV dosyasına kaydeder.\n",
    "\"\"\"\n",
    "df_answers = pd.DataFrame(data)\n",
    "df_answers.to_csv(\"student1_answers_clean_gpu_2.csv\", index=False)\n",
    "print(df_answers.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea2f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bu notebook, öğrenci modellerinin performansını değerlendirmek için kullanılır.\n",
    "Model cevapları ile gerçek referans cevapları arasındaki benzerliği çeşitli metriklerle ölçer.\n",
    "Kullanılan metrikler:\n",
    "- BLEU Score: N-gram eşleşmeleri üzerinden çeviri kalitesini ölçer. Daha uzun n-gram'lar için daha yüksek puan verir.\n",
    "- Jaccard Similarity: Kelime kümelerinin kesişiminin birleşimine oranı. Kelime bazlı benzerliği hesaplar.\n",
    "- TF-IDF Cosine Similarity: TF-IDF vektörleri arasındaki kosinüs açısı ile benzerliği ölçer. Kelime sıklığı ve önemini dikkate alır.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Gerekli kütüphanelerin import edilmesi.\n",
    "pandas: Veri manipülasyonu için.\n",
    "numpy: Sayısal işlemler için.\n",
    "matplotlib.pyplot: Grafikler için.\n",
    "datasets: Hugging Face dataset'lerini yüklemek için.\n",
    "sklearn: TF-IDF ve kosinüs benzerliği için.\n",
    "nltk: BLEU skoru için.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Konfigürasyon ayarları.\n",
    "Model cevaplarının bulunduğu CSV dosyasının yolu ve sütun isimleri.\n",
    "Soru ve referans sütunlarının isimleri.\n",
    "\"\"\"\n",
    "ANSWERS_CSV = \"/kaggle/input/datasetss/all_answers.csv\" # Model cevaplarının CSV dosya yolu\n",
    "\n",
    "MODEL_COLUMNS = {\n",
    "    \"student1\": \"student1_answer\",\n",
    "    \"student1_base\": \"student1_base_answer\",\n",
    "    \"student2\": \"student2_answer\",\n",
    "    \"student2_base\": \"student2_base_answer\",\n",
    "}\n",
    "\n",
    "QUESTION_COLUMN = \"question\"\n",
    "REFERENCE_COLUMN = \"answer\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Model cevaplarının güvenli bir şekilde yüklenmesi.\n",
    "CSV dosyasından veri okunur, hatalı satırlar atlanır.\n",
    "\"\"\"\n",
    "df_answers = pd.read_csv(\n",
    "    ANSWERS_CSV,\n",
    "    sep=None,\n",
    "    engine=\"python\",\n",
    "    encoding=\"utf-8\",\n",
    "    encoding_errors=\"replace\",\n",
    "    on_bad_lines=\"skip\"\n",
    ").fillna(\"\")\n",
    "\n",
    "print(\"Model answers loaded:\", df_answers.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Gerçek referans verilerinin Hugging Face dataset'inden yüklenmesi.\n",
    "behavior_data dataset'i kullanılır.\n",
    "\"\"\"\n",
    "behavior_ds = load_dataset(\"bylang/behavior_data\", split=\"train\")\n",
    "df_gt = behavior_ds.to_pandas()[[QUESTION_COLUMN, REFERENCE_COLUMN]].fillna(\"\")\n",
    "\n",
    "print(\"Ground truth loaded:\", df_gt.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Model cevapları ile referans verilerinin soru bazlı birleştirilmesi.\n",
    "Inner join ile sadece ortak sorular dahil edilir.\n",
    "\"\"\"\n",
    "df = pd.merge(\n",
    "    df_answers,\n",
    "    df_gt,\n",
    "    on=QUESTION_COLUMN,\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "print(\"Merged dataset:\", df.shape)\n",
    "display(df.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Metrik fonksiyonları tanımlanır.\n",
    "Bu fonksiyonlar, model cevaplarının referanslarla karşılaştırılması için kullanılır.\n",
    "\"\"\"\n",
    "\n",
    "smooth = SmoothingFunction().method1\n",
    "\n",
    "\n",
    "def bleu_score(ref, hyp):\n",
    "    \"\"\"\n",
    "    BLEU Score metriği.\n",
    "    N-gram eşleşmeleri üzerinden çeviri kalitesini ölçer.\n",
    "    1-gram'dan 4-gram'a kadar olan eşleşmeleri değerlendirir.\n",
    "    Smoothing function ile sıfır bölme önlenir.\n",
    "    \"\"\"\n",
    "    return sentence_bleu(\n",
    "        [ref.split()],\n",
    "        hyp.split(),\n",
    "        smoothing_function=smooth\n",
    "    )\n",
    "\n",
    "def jaccard_similarity(ref, hyp):\n",
    "    \"\"\"\n",
    "    Jaccard Similarity metriği.\n",
    "    Kelime kümelerinin kesişiminin birleşimine oranı.\n",
    "    Kelime bazlı benzerliği hesaplar.\n",
    "    Her iki metin boşsa 1.0 döndürür.\n",
    "    \"\"\"\n",
    "    ref_set = set(ref.split())\n",
    "    hyp_set = set(hyp.split())\n",
    "    if not ref_set and not hyp_set:\n",
    "        return 1.0\n",
    "    return len(ref_set & hyp_set) / len(ref_set | hyp_set)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "TF-IDF Cosine Similarity fonksiyonu.\n",
    "Referans ve hipotez metinlerini TF-IDF vektörlerine dönüştürür.\n",
    "Aralarındaki kosinüs benzerliğini hesaplar.\n",
    "\"\"\"\n",
    "\n",
    "def tfidf_cosine_scores(references, hypotheses):\n",
    "    \"\"\"\n",
    "    TF-IDF Cosine Similarity metriği.\n",
    "    TF-IDF vektörleri arasındaki kosinüs açısı ile benzerliği ölçer.\n",
    "    Kelime sıklığı ve önemini dikkate alır.\n",
    "    Diagonal elemanlar alınarak her referans-hipotez çifti için skor döndürür.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf = vectorizer.fit_transform(references + hypotheses)\n",
    "\n",
    "    ref_vecs = tfidf[:len(references)]\n",
    "    hyp_vecs = tfidf[len(references):]\n",
    "\n",
    "    return cosine_similarity(ref_vecs, hyp_vecs).diagonal()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Ana değerlendirme döngüsü.\n",
    "Her model için metrikleri hesaplar.\n",
    "BLEU, Jaccard ve TF-IDF Cosine skorlarını ortalar.\n",
    "\"\"\"\n",
    "\n",
    "results = {}\n",
    "\n",
    "refs = df[REFERENCE_COLUMN].tolist()\n",
    "\n",
    "for model_name, col in MODEL_COLUMNS.items():\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "\n",
    "    hyps = df[col].tolist()\n",
    "\n",
    "    exact = []\n",
    "    bleu = []\n",
    "    jaccard = []\n",
    "\n",
    "    for r, h in zip(refs, hyps):\n",
    "        bleu.append(bleu_score(r, h))\n",
    "        jaccard.append(jaccard_similarity(r, h))\n",
    "\n",
    "    cosine = tfidf_cosine_scores(refs, hyps)\n",
    "\n",
    "    results[model_name] = {\n",
    "        \"BLEU\": np.mean(bleu),\n",
    "        \"Jaccard\": np.mean(jaccard),\n",
    "        \"TFIDF-Cosine\": np.mean(cosine),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Sonuçların tablo olarak gösterilmesi ve CSV'ye kaydedilmesi.\n",
    "\"\"\"\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\nFINAL RESULTS\")\n",
    "display(results_df)\n",
    "\n",
    "results_df.to_csv(\"final_model_evaluation_results.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Sonuçların görselleştirilmesi.\n",
    "Bar grafikler, histogramlar ve boxplot ile metrik dağılımları gösterilir.\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "results_df.plot(kind=\"bar\")\n",
    "plt.title(\"Student vs Base Model Evaluation (Ground Truth: behavior_data)\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# metrik başına histogram\n",
    "for metric in results_df.columns:\n",
    "    plt.figure()\n",
    "    results_df[metric].plot(kind=\"hist\", bins=10)\n",
    "    plt.title(f\"Distribution of {metric}\")\n",
    "    plt.xlabel(metric)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Boxplot\n",
    "plt.figure(figsize=(12, 6))\n",
    "results_df.boxplot()\n",
    "plt.title(\"Metric Distribution Across Models\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEvaluation completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d4d6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets rouge-score scikit-learn pandas numpy seaborn matplotlib\n",
    "\n",
    "\"\"\"\n",
    "Veri analizi ve görselleştirme için gerekli kütüphaneleri import et\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "\n",
    "def jaccard_similarity(pred, ref):\n",
    "    \"\"\"\n",
    "    Jaccard benzerliği: Ortak kelimeler / Tüm kelimeler\n",
    "    Sonuç: 0 (tamamen farklı) - 1 (tamamen aynı)\n",
    "    \"\"\"\n",
    "    pred_set = set(pred.lower().split())\n",
    "    ref_set = set(ref.lower().split())\n",
    "    if len(pred_set | ref_set) == 0:\n",
    "        return 0.0\n",
    "    return len(pred_set & ref_set) / len(pred_set | ref_set)\n",
    "\n",
    "\n",
    "def tfidf_cosine_similarity(pred, ref, vectorizer):\n",
    "    \"\"\"\n",
    "    TF-IDF ile metinleri vektörleştir ve kosinüs benzerliğini hesapla\n",
    "    (Sık geçen kelimeler daha az ağırlık alır)\n",
    "    \"\"\"\n",
    "    tfidf = vectorizer.fit_transform([pred, ref])\n",
    "    return cosine_similarity(tfidf[0:1], tfidf[1:2])[0][0]\n",
    "\n",
    "\n",
    "def rouge_l_score(pred, ref, scorer):\n",
    "    \"\"\"\n",
    "    ROUGE-L skoru: En uzun ortak alt dizinin F-measure değeri\n",
    "    (Kelimelerin SİRASI önemlidir)\n",
    "    \"\"\"\n",
    "    return scorer.score(ref, pred)[\"rougeL\"].fmeasure\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "CSV dosyasından model cevaplarını yükle\n",
    "\"\"\"\n",
    "CSV_PATH = \"/kaggle/input/datasetss/all_answers.csv\"\n",
    "df = pd.read_csv(\n",
    "    CSV_PATH,\n",
    "    sep=None,\n",
    "    engine=\"python\",\n",
    "    encoding=\"utf-8\",\n",
    "    encoding_errors=\"replace\",\n",
    "    on_bad_lines=\"skip\"\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "HuggingFace'ten referans cevapları (ground truth) yükle\n",
    "\"\"\"\n",
    "behavior_ds = load_dataset(\"bylang/behavior_data\", split=\"train\", token=\"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\")\n",
    "ground_truth = behavior_ds[\"answer\"]\n",
    "\n",
    "\"\"\"\n",
    "Veri uzunluklarını eşitle\n",
    "\"\"\"\n",
    "min_len = min(len(df), len(ground_truth))\n",
    "df = df.iloc[:min_len]\n",
    "ground_truth = ground_truth[:min_len]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Model sütun adlarını tanımla\n",
    "\"\"\"\n",
    "model_columns = {\n",
    "    \"Student-1\": \"student1_answer\",\n",
    "    \"Student-1-Base\": \"student1_base_answer\",\n",
    "    \"Student-2\": \"student2_answer\",\n",
    "    \"Student-2-Base\": \"student2_base_answer\",\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Metrik hesaplama: Her modelin her cevabı için 3 skoru hesapla\n",
    "\"\"\"\n",
    "scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "rows = []\n",
    "\n",
    "for model_name, col in model_columns.items():\n",
    "    for pred, ref in zip(df[col].astype(str), ground_truth):\n",
    "        rows.append({\n",
    "            \"Model\": model_name,\n",
    "            \"ROUGE\": rouge_l_score(pred, ref, scorer),\n",
    "            \"Jaccard\": jaccard_similarity(pred, ref),\n",
    "            \"TFIDF_Cosine\": tfidf_cosine_similarity(pred, ref, tfidf_vectorizer)\n",
    "        })\n",
    "\n",
    "df_samples = pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Her modelin ortalama metrik skorlarını hesapla\n",
    "\"\"\"\n",
    "df_main = df_samples.groupby(\"Model\").mean().reset_index()\n",
    "print(\"\\n=== MODEL AVERAGE SCORES ===\")\n",
    "print(df_main)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Öğrenci modellerinin temel modellere kıyasla kazanımını hesapla\n",
    "Pozitif = iyileşme, Negatif = kötüleşme\n",
    "\"\"\"\n",
    "df_diff = pd.DataFrame({\n",
    "    \"Metric\": [\"ROUGE\", \"Jaccard\", \"TFIDF_Cosine\"],\n",
    "    \"Student-1 Gain\": (\n",
    "        df_main.loc[df_main.Model == \"Student-1\"].values[0][1:] -\n",
    "        df_main.loc[df_main.Model == \"Student-1-Base\"].values[0][1:]\n",
    "    ),\n",
    "    \"Student-2 Gain\": (\n",
    "        df_main.loc[df_main.Model == \"Student-2\"].values[0][1:] -\n",
    "        df_main.loc[df_main.Model == \"Student-2-Base\"].values[0][1:]\n",
    "    )\n",
    "})\n",
    "\n",
    "print(\"\\n=== STUDENT GAINS ===\")\n",
    "print(df_diff)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Bar grafikler: Her metrik için 4 modeli karşılaştır\n",
    "\"\"\"\n",
    "for metric in [\"ROUGE\", \"Jaccard\", \"TFIDF_Cosine\"]:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(data=df_main, x=\"Model\", y=metric)\n",
    "    plt.title(f\"{metric} Ortalama Karşılaştırması\")\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{metric.lower()}_barplot.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Boxplot: Her metrik için skorların dağılımını göster (Model bazında)\n",
    "\"\"\"\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(\n",
    "    data=df_samples.melt(\n",
    "        id_vars=\"Model\",\n",
    "        value_vars=[\"ROUGE\", \"Jaccard\", \"TFIDF_Cosine\"],\n",
    "        var_name=\"Metric\",\n",
    "        value_name=\"Score\"\n",
    "    ),\n",
    "    x=\"Metric\",\n",
    "    y=\"Score\",\n",
    "    hue=\"Model\"\n",
    ")\n",
    "\n",
    "plt.title(\"Örnek Bazlı Değerlendirme Skorlarının Dağılımı\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"boxplot.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Isı haritası: Tüm modellerin tüm metrik skorlarını matrix halinde göster\n",
    "\"\"\"\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.heatmap(\n",
    "    df_main.set_index(\"Model\"),\n",
    "    annot=True,\n",
    "    fmt=\".3f\",\n",
    "    cmap=\"viridis\",\n",
    "    linewidths=0.5,\n",
    "    linecolor=\"white\",\n",
    "    cbar_kws={\"label\": \"Similarity Score\"}\n",
    ")\n",
    "\n",
    "plt.title(\"Model Bazlı Anlamsal Değerlendirme Isı Haritası\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"heatmap_similarity.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Sonuçları CSV dosyalarına kaydet\n",
    "\"\"\"\n",
    "df_main.to_csv(\"average_metric_comparison.csv\", index=False)\n",
    "df_samples.to_csv(\"sample_level_scores.csv\", index=False)\n",
    "df_diff.to_csv(\"student_gains.csv\", index=False)\n",
    "\n",
    "print(\"\\nALL EVALUATION TABLES AND FIGURES GENERATED SUCCESSFULLY \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
